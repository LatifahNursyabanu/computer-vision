{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web_image_scraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#BING IMAGE DOWNLOADER"
      ],
      "metadata": {
        "id": "JF29NTsbvRiV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTgvBmmbu83c"
      },
      "outputs": [],
      "source": [
        "#install bing image downloader\n",
        "!pip install bing-image-downloader\n",
        "#Membuat direktori untuk menyimpan dataset\n",
        "!mkdir dataset_gambar\n",
        "#Downloading\n",
        "from bing_image_downloader import downloader\n",
        "downloader.download('barcode produk', limit=20, output_dir='dataset')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#simple scrapping\n",
        "import requests \n",
        "from PIL import Image \n",
        "from io import BytesIO \n",
        " \n",
        "res = requests.get(imageurl) \n",
        "img = Image(BytesIO(res.content)) \n",
        "img.save() "
      ],
      "metadata": {
        "id": "m8M1Ef9KvQ76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#GOOGLE IMAGE\n",
        "with selenium"
      ],
      "metadata": {
        "id": "n1WfZuPsvZ_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow\n",
        "!pip install selenium\n",
        "!pip install requests"
      ],
      "metadata": {
        "id": "NKbwj8T6v265"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#running in colab\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "\n",
        "from selenium import webdriver\n",
        "wd = webdriver.Chrome('/usr/bin/chromedriver')"
      ],
      "metadata": {
        "id": "7yjvCkCtv5ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#running in local\n",
        "#script1\n",
        "#https://www.youtube.com/watch?v=NBuED2PivbY&t=10s\n",
        "#https://github.com/techwithtim/Image-Scraper-And-Downloader/blob/main/tutorial.py\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import \n",
        "import requests\n",
        "import io\n",
        "import time\n",
        "from PIL import Image\n",
        "\n",
        "PATH = \"/content/capstone/chromedriver.exe\"\n",
        "wd = webdriver.Chrome(PATH)\n",
        "image_url = ''\n",
        "\n",
        "def get_images_from_google(wd, delay, max_images):\n",
        "  def scroll_down(wd):\n",
        "    wd.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
        "    time.sleep(delay)\n",
        "\n",
        "  url = ' '\n",
        "  wd.get(url)\n",
        "\n",
        "  image_urls = set()\n",
        "  skips = 0\n",
        "\n",
        "  while len(image_url) + skips < max_images:\n",
        "    scroll_down(wd)\n",
        "\n",
        "    thumbnails = wd.find_elements(By.CLASS_NAME, '')\n",
        "    for img in thumbnails[len(image_urls) + skips : max_images]:\n",
        "      try:\n",
        "        img.click()\n",
        "        time.sleep(delay)\n",
        "      except:\n",
        "        continue\n",
        "      \n",
        "      images = wd.find_elements(By.CLASS_NAME, '')\n",
        "      for image in images:\n",
        "        if image.get_attribute('src') and 'http' in image.get_attribute('src'):\n",
        "          image_urls.add(image.get_attribute('src'))\n",
        "          print(f'Found {len(image_urls)}')\n",
        "    \n",
        "    return image_urls\n",
        "\n",
        "def download_image(download_path, url, file_name):\n",
        "  try:\n",
        "    image_content = requests.get(url).content\n",
        "    image_file = io.BytesIO(image_content)\n",
        "    image = Image.opn(image_file)\n",
        "    file_path = download_path + file_name\n",
        "\n",
        "    with open(file_path, 'wb') as f:\n",
        "      image.save(f, 'JPEG')\n",
        "  \n",
        "    print('success')\n",
        "  except Exception as e:\n",
        "    print('FAILED')\n",
        "\n",
        "download_image('', image_url, 'test.jpg')\n",
        "get_images_from_google(wd,)\n",
        "urls = get_images_from_google(wd, 1, 6)\n",
        "\n",
        "for i, url in enumerate(urls):\n",
        "  download_image('imgs/', url, str(i)+ '.jpg')\n",
        "\n",
        "wd.quit() "
      ],
      "metadata": {
        "id": "EqnIThxav768"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#running in local\n",
        "#script2\n",
        "#https://www.youtube.com/watch?v=Yt6Gay8nuy0\n",
        "#https://github.com/ivangrov/Downloading_Google_Images/blob/main/webscraping_google_images/webscraping_google_images.py\n",
        "import bs4\n",
        "import requests\n",
        "from selenium import webdriver\n",
        "import os\n",
        "import time\n",
        "\n",
        "#creating a directory to save images\n",
        "folder_name = 'images'\n",
        "if not os.path.isdir(folder_name):\n",
        "    os.makedirs(folder_name)\n",
        "\n",
        "def download_image(url, folder_name, num):\n",
        "\n",
        "    # write image to file\n",
        "    reponse = requests.get(url)\n",
        "    if reponse.status_code==200:\n",
        "        with open(os.path.join(folder_name, str(num)+\".jpg\"), 'wb') as file:\n",
        "            file.write(reponse.content)\n",
        "\n",
        "\n",
        "chromePath=r'C:\\Users\\net51\\Documents\\MyPythonScripts\\Drivers\\chromedriver.exe'\n",
        "driver=webdriver.Chrome(chromePath)\n",
        "\n",
        "search_URL = \"https://www.google.com/search?q=car+parts&source=lnms&tbm=isch\"\n",
        "driver.get(search_URL)\n",
        "\n",
        "#//*[@id=\"islrg\"]/div[1]/div[1]\n",
        "#//*[@id=\"islrg\"]/div[1]/div[50]\n",
        "#//*[@id=\"islrg\"]/div[1]/div[25]\n",
        "#//*[@id=\"islrg\"]/div[1]/div[75]\n",
        "#//*[@id=\"islrg\"]/div[1]/div[350]\n",
        "\n",
        "\n",
        "a = input(\"Waiting...\")\n",
        "\n",
        "#Scrolling all the way up\n",
        "driver.execute_script(\"window.scrollTo(0, 0);\")\n",
        "\n",
        "page_html = driver.page_source\n",
        "pageSoup = bs4.BeautifulSoup(page_html, 'html.parser')\n",
        "containers = pageSoup.findAll('div', {'class':\"isv-r PNCib MSM1fd BUooTd\"} )\n",
        "\n",
        "print(len(containers))\n",
        "\n",
        "len_containers = len(containers)\n",
        "\n",
        "for i in range(1, len_containers+1):\n",
        "    if i % 25 == 0:\n",
        "        continue\n",
        "\n",
        "    xPath = \"\"\"//*[@id=\"islrg\"]/div[1]/div[%s]\"\"\"%(i)\n",
        "\n",
        "    previewImageXPath = \"\"\"//*[@id=\"islrg\"]/div[1]/div[%s]/a[1]/div[1]/img\"\"\"%(i)\n",
        "    previewImageElement = driver.find_element_by_xpath(previewImageXPath)\n",
        "    previewImageURL = previewImageElement.get_attribute(\"src\")\n",
        "    #print(\"preview URL\", previewImageURL)\n",
        "\n",
        "\n",
        "    #print(xPath)\n",
        "\n",
        "\n",
        "    driver.find_element_by_xpath(xPath).click()\n",
        "    #time.sleep(3)\n",
        "\n",
        "    #//*[@id=\"islrg\"]/div[1]/div[16]/a[1]/div[1]/img\n",
        "\n",
        "    #input('waawgawg another wait')\n",
        "\n",
        "    # page = driver.page_source\n",
        "    # soup = bs4.BeautifulSoup(page, 'html.parser')\n",
        "    # ImgTags = soup.findAll('img', {'class': 'n3VNCb', 'jsname': 'HiaYvf', 'data-noaft': '1'})\n",
        "    # print(\"number of the ROI tags\", len(ImgTags))\n",
        "    # link = ImgTags[1].get('src')\n",
        "    # #print(len(ImgTags))\n",
        "    # #print(link)\n",
        "    #\n",
        "    # n=0\n",
        "    # for tag in ImgTags:\n",
        "    #     print(n, tag)\n",
        "    #     n+=1\n",
        "    # print(len(ImgTags))\n",
        "\n",
        "    #/html/body/div[2]/c-wiz/div[3]/div[2]/div[3]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[1]/div[2]/div[1]/a/img\n",
        "\n",
        "    #It's all about the wait\n",
        "\n",
        "    timeStarted = time.time()\n",
        "    while True:\n",
        "\n",
        "        imageElement = driver.find_element_by_xpath(\"\"\"//*[@id=\"Sva75c\"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[1]/div[2]/div[1]/a/img\"\"\")\n",
        "        imageURL= imageElement.get_attribute('src')\n",
        "\n",
        "        if imageURL != previewImageURL:\n",
        "            #print(\"actual URL\", imageURL)\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            #making a timeout if the full res image can't be loaded\n",
        "            currentTime = time.time()\n",
        "\n",
        "            if currentTime - timeStarted > 10:\n",
        "                print(\"Timeout! Will download a lower resolution image and move onto the next one\")\n",
        "                break\n",
        "\n",
        "\n",
        "    #Downloading image\n",
        "    try:\n",
        "        download_image(imageURL, folder_name, i)\n",
        "        print(\"Downloaded element %s out of %s total. URL: %s\" % (i, len_containers + 1, imageURL))\n",
        "    except:\n",
        "        print(\"Couldn't download an image %s, continuing downloading the next one\"%(i))\n",
        "\n",
        "    #//*[@id=\"Sva75c\"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[1]/div[2]/div[1]/a/img\n",
        "    #//*[@id=\"Sva75c\"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[1]/div[2]/div[1]/a/img\n"
      ],
      "metadata": {
        "id": "Fqc1RDBswVFH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}